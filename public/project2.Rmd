---
title: "Survey of Labour and Income Dynamics for Ontario (1994)"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(carData)
library(lmtest)
library(sandwich)
library(MASS)
library(dplyr)
library(glmnet)
```

### Introduction

This SLID dataset is from carData. It contains survey data from a 1994 Canadian Survey of Labour and Income Dynamics for the province of Ontario, Canada. 7425 cases and 5 variables-- hourly wage from all jobs, number of years of education, age in years, sex (male, female), and language (French, English, Other). The missing data points have been removed. 


```{R}
data<-carData::SLID
data<-data%>%remove_missing()
```

####Part 1.


```{R}
man1<-manova(cbind(data$wages,data$education,data$age)~sex, data=data)
summary(man1)

summary.aov(man1)

data%>%group_by(sex)%>%summarize(mean(wages))
pairwise.t.test(data$wages,data$sex, p.adj="none")

```


1 MANOVA, 3 ANOVAs, 1 t-test= 5 tests. 

Probability of Making at least 1 type 1 error using alpha = 0.05: 
1 - (0.95)^5 = 0.2262191

Bonferroni correction alpha= 0.05/5 = 0.01.

A one-way multivariate analysis was conducted to determine the effect of sex on three dependent variables (wages, education, and age). Significant differences were found among the two sexes on the three dependent measures F(1,3985)=92.08, p-value << 0.0001.

Univariate analyses of variance (ANOVAs) for each dependent variables were conducted as a follow up test, using the Bonferroni method to control type 1 error rates for multiple comparions. The ANOVAs for wages were significant, F(1,3985)=194.64, p <<0.0001.

Since only one dependent variable tested significant and this only had two possible outcomes, we konw which groups differed, but post hoc analysis was performed and it showed a signicant difference in terms of wages even after adjusting for multiple comparisons. 

This data was a survey with individuals with missing data points removed, making it deviate from random sampling. This leads us to believe that since there are a lot of other assumptions such as multivariate normality of DVs, homogeneity of within group covariance matrices, linear relationships among DVs, no extreme univariate or multivariate outliers, and no multicollinearity, we have to doubt that all of these assumptions are met.  

####Part 2.


```{R}
 data%>% group_by(sex) %>%
  summarize(means=mean(wages)) %>% summarize(`mean_diff:` = diff(means))

rand_dist<-vector()

for(i in 1:5000){
new<-data.frame(wages=sample(data$wages),condition=data$sex) 
rand_dist[i]<-mean(new[new$condition=="Male",]$wages)-
              mean(new[new$condition=="Female",]$wages)}

mean(rand_dist>3.395609	)*2 #pvalue

{hist(rand_dist,main="Mean Difference in Wages for Males vs. Females",ylab="Frequency");
  abline(v = 8.10 ,col="red")}
```


H0: Mean wages is the same for females vs. males in Ontario.

Ha: Mean wages is different for males vs. females 

The p-value corresponds to the probability of observing a mean difference as extreme as the one we got under this "randomization distribution", which was calculated to be 0<<0.001, meaning there is a significant difference in mean wages between males and females of Ontario.

####Part 3.

```{R}

data$age_c <- data$age - mean(data$age)
data$education_c <- data$education - mean(data$education)
fit <- lm(wages ~ age_c * education_c, data = data)
summary(fit)

qplot(x = education_c, y = wages, color = age_c, data = data) +
 stat_smooth(method = "lm", se = FALSE, fullrange = TRUE)


new1 <- data
new1$age_c <- mean(data$age_c)
new1$mean <- predict(fit,new1)
new1$age_c <- mean(data$age_c)+sd(data$age_c)
new1$plus.sd <- predict(fit,new1)
new1$age_c <- mean(data$age_c)-sd(data$age_c)
new1$minus.sd <- predict(fit,new1)
newint <- new1 %>%dplyr:: select(wages,
education_c, mean, plus.sd, minus.sd) %>% 
gather(age, value, -wages, -education_c)

mycols<-c("#619CFF","#F8766D","#00BA38")
names(mycols)<-c("-1 sd","mean","+1 sd")
mycols=as.factor(mycols)

ggplot(data,aes(education_c,wages),group=mycols)+
  geom_point()+
  geom_line(data=new1,aes(y=mean,color="mean"))+
  geom_line(data=new1,aes(y=plus.sd,color="+1 sd"))+
  geom_line(data=new1,aes(y=minus.sd,color="-1 sd"))+
  scale_color_manual(values=mycols)+
  labs(color="age")+
  theme(legend.position=c(.9,.2))

resids<-fit$residuals; fitvals<-fit$fitted.values
ggplot()+geom_point(aes(fitvals,resids))+
  geom_hline(yintercept=0, col="red")
bptest(fit)

ggplot()+geom_histogram(aes(resids),bins=20)

library(sandwich) 
coeftest(fit)[,1:2]

coeftest(fit, vcov=vcovHC(fit))[,1:2]

fit2 <- lm(wages~ age_c+education_c, data = data)
summary(fit2)

anova(fit, fit2, test = "LRT")
```
 wage= intercept+A(age_c) + B(education_c) + C(age_c*education_c)

Since this is mean centered, the intercept means that a person of average age and education, the hourly wage earned is 15.6177 canadian dollars. For every one year increase in age at an average education, wage increases by 0.27 dollars. For every one year increase in education at an average age, wage increases by 0.835 dollars. The interaction coefficient is the difference in wages (0.02 dollars) depending on both the effects of education_c and age_c. Interaction effect exists when the effect of an independent variable on a dependent variable changes, depending on the value of one or more other independent variables. All 3 assumptions (linearity, normality and homoskedasticity) failed graphically and by hypothesis test. Using robust SEs, standard errors decreased for education_c. The standard error increased for intercept, age_c and the interaction term. 25.76% of variation in wages is explained by age_c and education_c with interaction.  Using LRT to compare the interaction model and no interaction models, the non-interaction model is better with a p-value<<0.0001, so we can reject the null hypothesis.




####Part 4.

```{R}
samp_distn<-replicate(5000, {
  boot_dat<-data[sample(nrow(data),replace=TRUE),]
  fit<-lm(wages~ age_c * education_c ,data=boot_dat) 
  coef(fit)
})

## Estimated SEs
samp_distn%>%t%>%as.data.frame%>%summarize_all(sd)

```


Compared to the original SEs, intercept, age_c, interaction SEs increased whereas education_c decreased. Compared to the robust SEs, intercept increased in SE whereas age_c, education_c, and interaction SEs decreased.

####Part 5. 

```{R}
data$y <- ifelse(data$sex == "Male", 1, 0)

fit3<-glm(y~wages+education,data=data,family=binomial(link="logit"))
coeftest(fit3)

exp(coef(fit3))

probs<-predict(fit3,type="response") 
class_diag<-function(probs,truth){
  
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
ppv=tab[2,2]/rowSums(tab)[2]

if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1

  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  
  data.frame(acc,sens,spec,ppv,auc)
}
class_diag(probs, data$y)

table(truth=data$y, prediction=as.numeric(probs>.5))%>%addmargins
#accuracy 
(1341+1110)/3987
#TPR (sensitivity)
(1110/1986)
#TNR (specificity)
1341/2001
#precision (ppv)
1110/1770

data$logit<-predict(fit3)
ggplot(data,aes(logit, fill=sex))+geom_density(alpha=.3)+
  geom_vline(xintercept=0,lty=2)

library(plotROC) 
ROCplot<-ggplot(data)+geom_roc(aes(d=y,m=probs), n.cuts=0) 
ROCplot
calc_auc(ROCplot)

set.seed(1234)
k=5
data1<-data[sample(nrow(data)),] 
folds<-cut(seq(1:nrow(data)),breaks=k,labels=F) 
diags<-NULL
for(i in 1:k){
 train<-data1[folds!=i,]
 test<-data1[folds==i,]
 truth<-test$y
 fit4<-glm(y~wages + education,data=train,family="binomial")
 probs<-predict(fit4,newdata = test,type="response")
 diags<-rbind(diags,class_diag(probs,truth))
}
apply(diags,2,mean)
```

From the logistic regression, we see that at 0 years age and education, the odds of being a male is multiplied by a factor of 0.904. For 1 dollar in wages, the odds of being male is multiplied by a factor of 1.07. For 1 year of education, the odds of being male is multiplied by a factor of 0.93. From the confusion matrix, we have accuracy of 0.615, sensitivity of 0.559, specificity of 0.67, and precision of 0.627. The grey area in the visualization of our logistic regression model shows misclassified area in the grey. The portion to the right of 0 is the proportion of females predicted males (false positive) and the grey area to the left of 0 is proportion males predicted as female (false negative). The AUC of our ROC plot is 0.64, meaning this is a bad ROC plot. It is difficult to predict the odds of being male from wages and education alone. With a 10 fold CV, the accuracy was 0.62, sensitivity was 0.56, specificity was 0.67, and ppv was 0.628.

####Part 6.



```{R}

y<-as.matrix(data$y)
x<-data %>%
  dplyr::select(1:3) %>%
  mutate_all(scale) %>%
  as.matrix
cv<-cv.glmnet(x,y)
lasso1<-glmnet(x,y,lambda=cv$lambda.1se)
coef(lasso1)

set.seed(1234)
k= 10
data1 <- data[sample(nrow(data)),] 
folds<-cut(seq(1:nrow(data)),breaks=k,labels=F) 
diags<-NULL
for(i in 1:k){
 train<-data1[folds!=i,]
 test<-data1[folds==i,]
 truth<-test$y
 fit5<-glm(y~wages,data=train,family="binomial")
 probs<-predict(fit5,newdata = test,type="response")
 diags<-rbind(diags,class_diag(probs,truth))
}
diags%>%summarize_all(mean)

```

This Lasso regression shows that wages, education, and age are the most significant predictors of sex. The AUC from this model is less (0.632) than that of part 5 (0.64), making the logistic regression a better fit for the data. 
